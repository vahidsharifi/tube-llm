[32mINFO[39m:     Started server process [[36m95397[39m]
[32mINFO[39m:     Waiting for application startup.
[32mINFO[39m:     Application startup complete.
3 DOCUMENTS LOADED
[31mERROR[39m:    Exception in ASGI application
Traceback (most recent call last):
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py", line 426, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/fastapi/applications.py", line 290, in __call__
    await super().__call__(scope, receive, send)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py", line 79, in __call__
    raise exc
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py", line 68, in __call__
    await self.app(scope, receive, sender)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py", line 20, in __call__
    raise e
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py", line 17, in __call__
    await self.app(scope, receive, send)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/routing.py", line 718, in __call__
    await route.handle(scope, receive, send)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/routing.py", line 276, in handle
    await self.app(scope, receive, send)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/starlette/routing.py", line 66, in app
    response = await func(request)
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/fastapi/routing.py", line 241, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/fastapi/routing.py", line 167, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/vahid/GitHub/lc_qa_chat/app.py", line 61, in answer_question
    my_table = wandb.Table(columns=list(response.keys()), data=list(response.values()))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/wandb/data_types.py", line 290, in __init__
    self._init_from_list(data, columns, optional, dtype)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/wandb/data_types.py", line 315, in _init_from_list
    self.add_data(*row)
  File "/Users/vahid/GitHub/lc_qa_chat/venv/lib/python3.11/site-packages/wandb/data_types.py", line 466, in add_data
    raise ValueError(
ValueError: This table expects 3 columns: ['question', 'answer', 'sources'], found 24
Response:  {'question': 'how to fine-tune llama-2', 'answer': 'To fine-tune llama-2, you need to use Q Laura and push the model to the Hugging Face model Hub. You can start with a quick demo and then use a custom dataset in CSV format with three columns: instruction, input, and output. The model to be used is llama2 70b chat from Hugging Face. Make sure your Hugging Face account and email address match the email provided on The Meta website. The fine-tuning process involves training llama2 based on French quotes and using it to generate French text.', 'sources': 'https://www.youtube.com/watch?v=6iHVJyX2e50, https://www.youtube.com/watch?v=eeM6V5aPjhk, https://www.youtube.com/watch?v=3fsn19OI_C8'}
[32mINFO[39m:     127.0.0.1:54763 - "[1mGET /answer/?question=how%20to%20fine-tune%20llama-2 HTTP/1.1[22m" [91m500 Internal Server Error
[32mINFO[39m:     Shutting down
[32mINFO[39m:     Waiting for application shutdown.
[32mINFO[39m:     Application shutdown complete.
[32mINFO[39m:     Finished server process [[36m95397[39m]